{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from dictionary_construction.const import FIX_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_html = r'grammar_pages'\n",
    "pages_list = os.listdir(path_to_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunpro_adj_options(page_list: list) -> set:\n",
    "    adj_options = set()\n",
    "    for page in page_list:\n",
    "        soup = BeautifulSoup(open(os.path.join(path_to_html, page), 'r', encoding='utf-8'), 'html.parser')\n",
    "        title = soup.select('ul h4')\n",
    "        labels = soup.select('ul > li > p')\n",
    "        for t, l in zip(title, labels):\n",
    "            if t.get_text(strip=True) == \"Part of Speech\":\n",
    "                adj_options.add(l.get_text(strip=True))\n",
    "    return sorted(adj_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determaine_reading(soup: BeautifulSoup) -> str:\n",
    "    try:\n",
    "        reading = soup.select_one('h1 rt').get_text(strip=True)\n",
    "    except:\n",
    "        reading = soup.select_one('h1').get_text(strip=True)\n",
    "    return reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_explination(soup: BeautifulSoup) -> str:\n",
    "    main_div = soup.find('div', class_='bp-ddw bp-writeup-body prose')\n",
    "\n",
    "    # Remove example sentence sections by specifying the classes you want to skip\n",
    "    if main_div is None:\n",
    "        return None\n",
    "    for example_section in main_div.find_all(class_=['writeup-example--japanese', 'writeup-example--english']):\n",
    "        example_section.decompose()  # This removes the element from the tree\n",
    "\n",
    "    # Extract the remaining text\n",
    "    explination = main_div.get_text(separator=' ', strip=True)\n",
    "    return explination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_pos(soup: BeautifulSoup) -> str:\n",
    "    title = soup.select('ul h4')\n",
    "    labels = soup.select('ul > li > p')\n",
    "    for t, l in zip(title, labels):\n",
    "        if t.get_text(strip=True) == \"Part of Speech\":\n",
    "            result = l.get_text(strip=True)\n",
    "            result = FIX_POS.get(result)\n",
    "            if result:\n",
    "                return result\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown POS: {result}\")\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JLPT_level(soup: BeautifulSoup) -> str:\n",
    "    title = soup.find('title').get_text(strip=True)\n",
    "    # Extract the JLPT level from the title\n",
    "    jlpt = title.split('JLPT')[1].strip(') | Bunpro')\n",
    "    if jlpt not in ['N5', 'N4', 'N3', 'N2', 'N1', 'N0']:\n",
    "        raise ValueError(f\"Unknown JLPT level: {jlpt}\")\n",
    "    return jlpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_latin_chars(text):\n",
    "    # Regular expression to match Latin characters (a-z, A-Z)\n",
    "    return re.sub(r'[a-zA-Z,]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_kanji(text):\n",
    "    # Regular expression for Kanji (CJK Unified Ideographs)\n",
    "    kanji_pattern = re.compile(r'[\\u4E00-\\u9FFF]')\n",
    "    return bool(kanji_pattern.search(str(text)))  # Convert text to string in case of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_entry(subject, reading, part_of_speech, definition, explanation, matchup=10, jlpt=\"N5\") -> str:\n",
    "    data = [\n",
    "        subject,  # Kanji\n",
    "        reading,   # Kana\n",
    "        part_of_speech,    # Part of speech 1\n",
    "        part_of_speech,    # Part of speech 2\n",
    "        matchup, \n",
    "        [\n",
    "            {\"type\": \"structured-content\", \"content\": [\n",
    "                \"【 Meaning 】\",\n",
    "                {\n",
    "                    \"tag\": \"div\",\n",
    "                    \"style\": {\"marginLeft\": 1},\n",
    "                    \"content\": definition\n",
    "                },\n",
    "                \"【 Explination 】\",\n",
    "                {\n",
    "                    \"tag\": \"div\",\n",
    "                    \"style\": {\"marginLeft\": 1},\n",
    "                    \"content\": explanation\n",
    "                },\n",
    "                \"【 Example sentences 】\",\n",
    "                {\n",
    "                    \"tag\": \"ol\",\n",
    "                    \"content\": [\n",
    "                        {\"tag\": \"li\", \"style\": {\"listStyleType\": \"'①'\"}, \"content\": \"例文 1\\nSentence 1\"},\n",
    "                        {\"tag\": \"li\", \"style\": {\"listStyleType\": \"'②'\"}, \"content\": \"Sentence 2\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        1,  # Some boolean flag\n",
    "        jlpt  # JLPT Level\n",
    "    ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_duplicate_rows(row):\n",
    "    # Split the 'subject' column by the '・' character\n",
    "    splits = row['subject'].split('・')\n",
    "    # Duplicate the row for each split\n",
    "    return pd.DataFrame({\n",
    "        'subject': splits,\n",
    "        'part_of_speech': [row['part_of_speech']] * len(splits),\n",
    "        'definition': [row['definition']] * len(splits),\n",
    "        'explanation': [row['explanation']] * len(splits),\n",
    "        'JLPT': [row['JLPT']] * len(splits),\n",
    "        'contains_kanji': [row['contains_kanji']] * len(splits)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to create a mapping from bunpro to yomichan POS. Acessing all the pages takes a while so the results are saved in a dictionary in const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Used to create a mapping from bunpro to yomichan POS\n",
    "# # results are saved in a dictionary in const.py\n",
    "# bun_opt = bunpro_adj_options(pages_list)\n",
    "# yomi_pos = ['adj-na','adj-na', 'adv', 'aux-v', 'prt', 'exp', 'adj-na', 'n' , 'prt', 'pn', 'v-unspec']\n",
    "# fix_pos = {key: value for key, value in zip(bun_opt, yomi_pos)}\n",
    "# print(fix_pos)\n",
    "# assert len(bun_opt) == len(yomi_pos), \"Lengths of bunpro and yomichan POS lists are not equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an exmple page to find location of information with b4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_of_interest = '-%E3%82%93%E3%81%A7%E3%81%99-%E3%81%AE%E3%81%A7%E3%81%99.html'\n",
    "index = 0\n",
    "for i, page in enumerate(pages_list):\n",
    "    if page == page_of_interest:\n",
    "        index = i\n",
    "        break\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read page with BeautifulSoup\n",
    "soup = BeautifulSoup(open(os.path.join(path_to_html, page_of_interest), 'r', encoding='utf-8'), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URL to link back to the original page\n",
    "url = soup.select_one('head > link[rel=\"canonical\"]')['href']\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to find the grammar topic, might not be the best way\n",
    "subject = soup.find('h1').get_text(strip=True)\n",
    "print(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_def = soup.select_one('p.line-clamp-1').get_text(strip=True)\n",
    "print(f\"A simple definition of the grammar point is: {simple_def}\")\n",
    "explination = extract_explination(soup)\n",
    "print(f\"The explination of the grammar point is: {explination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an exmaple entry in a dict. This dict can be passed straight to a JSON proccessing function or into a pandas dataframe for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_contents = {\n",
    "    \"subject\": soup.find('h1').get_text(strip=True),\n",
    "    \"reading\": determaine_reading(soup),\n",
    "    \"part_of_speech\": determine_pos(soup),\n",
    "    \"definition\": soup.select_one('p.line-clamp-1').get_text(strip=True),\n",
    "    \"explanation\": extract_explination(soup),\n",
    "    \"link\": soup.select_one('head > link[rel=\"canonical\"]')['href']\n",
    "}\n",
    "print(entry_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the JLPT level\n",
    "jlpt = JLPT_level(soup)\n",
    "print(jlpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataframe from the HTML files. This should be a temporary measure for examining the data. Future extraction flow show go straight from HTML to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(entry_contents, index=[0])\n",
    "for page in pages_list:\n",
    "    soup = BeautifulSoup(open(os.path.join(path_to_html, page), 'r', encoding='utf-8'), 'html.parser')\n",
    "    entry_contents = {\n",
    "        \"subject\": remove_latin_chars(soup.find('h1').get_text(strip=True).split(' ')[0]),\n",
    "        \"part_of_speech\": determine_pos(soup),\n",
    "        \"definition\": soup.select_one('p.line-clamp-1').get_text(strip=True),\n",
    "        \"explanation\": extract_explination(soup),\n",
    "        \"JLPT\": JLPT_level(soup)\n",
    "        \n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame(entry_contents, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a hard time with getting the readings. Possbily have to hand-write the kanji readings. Function to ID what entires to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_kanji'] = df['subject'].apply(contains_kanji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the rows based on '・' and duplicate other columns\n",
    "expanded_df = pd.concat(df.apply(split_and_duplicate_rows, axis=1).tolist(), ignore_index=True)\n",
    "expanded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.to_csv('example_extration.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = pd.read_csv('bunpro_entries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with subject = ~\n",
    "dict = dict[dict['subject'] != '~']\n",
    "dict[dict['definition'].duplicated(keep=False)].sort_values('definition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing dict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_construction.Entry import Dictionary_Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Dictionary_Entry(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "んだ・んです\n"
     ]
    }
   ],
   "source": [
    "print(test.subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
